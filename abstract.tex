In recent years, the field of machine learning effectively leveraged increased computing power. Through increasingly complex topologies, performance breakthroughs were achieved in image classification, machine translation, text and image generation and other applications. However, it was unfortunately observed that this generally came at a large expense of network calibration or the confidence networks have in their predictions. This presents a dilemma to the subfield of neuro-symbolic integration for probablistic inference, where ideally neural networks' excellent learning capabilities are combined with necessarily sound calibration. We implement different calibration methodologies in the DeepProbLog probablistic logic programming language and benchmark them in a few visual workload case studies. We conclude which methodology is appropriate in this context in which circumstances.