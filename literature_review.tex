\section{Review of the calibration problem and calibration methods}
In this section we will summarize \cite{guo2017calibration}'s findings and propositions on the topic of neural network calibration.

\subsection{The general problem of neural network model calibration degradation}
\cite{guo2017calibration} discovered that they could not confirm \cite{niculescu2005predicting}'s observation that neural networks (for binary classification) were typically well-calibrated for the latest generation of deep neural network models. \cite{lecun1998gradient}'s 5-layer LeNet was compared to \cite{he2016deep}'s 110 layer ResNet on the CIFAR-100 dataset. This comparison is visualized in figure \ref{fig:lenet_vs_resnet_calibration_guo_et_al}, where the top row shows prediction confidence histograms and the bottom row shows reliability diagrams (RDs). RDs show a model's accuracy as a function of its confidence. A perfectly callibrated model has a 45Â° angled line, i.e. the identity function, as its RD. Figure \ref{fig:lenet_vs_resnet_calibration_guo_et_al} shows a striking overconfidence of the ResNet on CIFAR-100 while showing the LeNet has an almost perfect RD on it. \cite{guo2017calibration} thus proposes that the modern ResNet has gained in accuracy over the older LeNet by, through some mechanism, sacrificing calibration. Through abductive reasoning \cite{guo2017calibration} hypothesizes and substantiates that this is a general and explainable evolutionary trend in artificial neural network R\&D that one could call calibration degradation. 

\begin{figure}[htbp!]
  \centering
  \includegraphics[width=0.6\linewidth]{images/lenet_vs_resnet_calibration_guo_et_al.jpg}
  \caption[5-layer LeNet's vs. 110-layer ResNet's calibration.]{5-layer LeNet (left) vs. 110-layer ResNet (right) calibration.}
  \label{fig:lenet_vs_resnet_calibration_guo_et_al}
  \source{\cite{guo2017calibration}}
\end{figure}
\newpage

\subsection{Formal calibration definitions and metrics}
\cite{guo2017calibration} provide a rigorous definition of and enumerate several metrics for the calibration of supervised multiclass classification.

\subsubsection{Notation and the formal perfect calibration definition}
The input $X \in \mathcal{X}$ and label $Y \in \mathcal{Y}=\{1, \ldots, K\}$ are random variables that follow a ground truth joint distribution $\pi(X, Y)=$ $\pi(Y \mid X) \pi(X)$. Let $h$ be a neural network with $h(X)=$ $(\hat{Y}, \hat{P})$, where $\hat{Y}$ is a class prediction and $\hat{P}$ is its confidence or self-estimated probability of correctness. Neural network $h$ is theoretically defined as perfectly calibrated if and only if equation \ref{eqn:perfect_calibration_in_theory} holds over the ground truth joint distribution $\pi(X, Y)$.
\begin{equation}
  \label{eqn:perfect_calibration_in_theory}
  \mathbb{P}(\hat{Y}=Y \mid \hat{P}=p)=p, \quad \forall p \in[0,1]
\end{equation}

Equation \ref{eqn:perfect_calibration_in_theory} is not practically applicable: $p$ is a continuous variable, therefore equation \ref{eqn:perfect_calibration_in_theory}'s satisfaction cannot be checked in finite time. Equation \ref{eqn:perfect_calibration_in_practice} can instead be used in practice. It is a binning method where predictions are grouped into $M$ equally sized bins over the interval $\interval{0}{1}$ based on their associated confidence.

\begin{align}
  \label{eqn:perfect_calibration_in_practice}
  \operatorname{acc}\left(B_{m}\right) &= \operatorname{conf}\left(B_{m}\right), \quad \forall m \in[0,M] \\ \nonumber
  \text{where}~\operatorname{acc}\left(B_{m}\right) &= \frac{1}{\left|B_{m}\right|} \sum_{i \in B_{m}} \mathbf{1}\left(\hat{y}_{i}=y_{i}\right), \\ \nonumber
  \operatorname{conf}\left(B_{m}\right) &= \frac{1}{\left|B_{m}\right|} \sum_{i \in B_{m}} \hat{p}_{i}, \\ \nonumber
  B_{m} &= \parbox[t]{8cm}{\raggedright The set of indices of samples whose prediction confidence falls into the interval $I_{m}$,} \\ \nonumber
  I_{m} &= \left[\frac{m-1}{M}, \frac{m}{M}\right], \\ \nonumber
  \hat{y}_{i} &= \text{Predicted class label of sample $i$}, \\ \nonumber
  y_{i} &= \text{True class label of sample $i$}, \\ \nonumber
  \hat{p}_{i} &= \text{Confidence of the class prediction of sample $i$}
\end{align}

$\operatorname{acc}\left(B_{m}\right)$ is an unbiased and consistent estimator of $\mathbb{P}\left(\hat{Y}=Y \mid \hat{P} \in I_{m}\right)$ by basic probability theory.

\subsubsection{Calibration metrics}
To make the criterion of perfect calibration, which is a goal that can rarely be reached in practice, useful we need to combine it with a quantitative measure of how close a model is to meeting it. Such a measure then allows us to bring a model ever closer to being perfectly calibrated instead of only to determine whether or not it is so. Three main metrics are layed out by \cite{guo2017calibration} and discussed here. We will call these metrics miscalibration metrics.

\paragraph{Expected Calibration Error (ECE)}
The expected calibration error (ECE), defined by equation \ref{eqn:ece_theoretical}, quantifies miscalibration as the expected absolute difference between the true accuracy of the model given its probabilistic confidence and its probabilistic confidence.
\begin{equation}
  \label{eqn:ece_theoretical}
  \mathrm{ECE}=\underset{\hat{P}}{\mathbb{E}}[|\mathbb{P}(\hat{Y}=Y \mid \hat{P}=p)-p|]
\end{equation}
Equation \ref{eqn:ece_theoretical} cannot be evaluated in practice because $\hat{P}$ is a continuous variable. Therefore equation \ref{eqn:ece_practical}, an approximation-by-binning method similar to equation \ref{eqn:perfect_calibration_in_practice} is used in practice.
\begin{equation}
  \label{eqn:ece_practical}
  \mathrm{ECE}=\sum_{m=1}^{M} \frac{\left|B_{m}\right|}{n}\left|\operatorname{acc}\left(B_{m}\right)-\operatorname{conf}\left(B_{m}\right)\right| \text {, }
\end{equation}
The expected calibration error metric is appropriate for use in model calibration optimization if calibration outliers are acceptable. ECE is ideally 0.

\paragraph{Maximum Calibration Error (MCE)}
The maximum calibration error (MCE), defined by equation \ref{eqn:mce_theoretical}, quantifies miscalibration as the maximum absolute difference between the true accuracy of the model given its probabilistic confidence and its probabilistic confidence.
\begin{equation}
  \label{eqn:mce_theoretical}
  \mathrm{MCE}=\max _{p \in[0,1]}|\mathbb{P}(\hat{Y}=Y \mid \hat{P}=p)-p| \text {. }
\end{equation}
Equation \ref{eqn:mce_theoretical} cannot be evaluated in practice because $\hat{P}$ is a continuous variable. Therefore equation \ref{eqn:mce_practical}, an approximation-by-binning method similar to equation \ref{eqn:perfect_calibration_in_practice} is used in practice.
\begin{equation}
  \label{eqn:mce_practical}
  \mathrm{MCE}=\max _{m \in\{1, \ldots, M\}}\left|\operatorname{acc}\left(B_{m}\right)-\operatorname{conf}\left(B_{m}\right)\right|
\end{equation}
The maximum calibration error metric is appropriate for use in model calibration optimization if calibration outliers are problematic. MCE is ideally 0.

\paragraph{Negative log-likelihood (NLL)}
Negative log-likelihood (also commonly known as cross-entropy loss in deep learning) $\mathcal{L}$ is a standard measure of probabilistic model fit. Equation \ref{eqn:nll_theoretical} gives its definition.

\begin{align}
  \label{eqn:nll_theoretical}
  \mathcal{L} &= -\sum_{i=1}^{n} \log \left(\hat{\pi}\left(y_{i} \mid \mathbf{x}_{i}\right)\right) \\ \nonumber
  \text{where}~\hat{\pi}(Y \mid X) &= \text{The probabilistic model under evaluation,} \\ \nonumber
  n &= \text{The number of samples}
\end{align}
Minimizing equation \ref{eqn:nll_theoretical} brings $\hat{\pi}(Y \mid X)$ ever closer to the true probability distribution. If negative log-likelihood is 0, $\hat{\pi}(Y \mid X)$ is the ground truth probability distribution. We do not use negative log-likelihood in practice to calibrate deep learning models because in this context \cite{guo2017calibration} show that it is too susceptible to overfitting.

\subsection{Calibration methods}
\cite{guo2017calibration} provide and compare several calibration methods we will discuss here.

\subsubsection{Histogram binning}
Histogram binning is a non-parametric calibration method where all uncalibrated prediction confidences $\hat{p}_{i}$ of the dataset are divided into bins $B_{1}, \ldots, B_{M}$. Each bin is assigned a calibrated prediction confidence $\theta_{m}$ such that if $\hat{p}_{i}$ is assigned to bin $B_{m}$, then $\hat{q}_{i}=\theta_{m}$ where $\hat{q}_{i}$ stands for sample $i$'s calibrated prediction confidence. The $\theta_{m}$'s are chosen to minimize the bin-wise squared loss as defined by equation \ref{eqn:histogram_binning} in the case of binary classification.
\begin{align}
  \label{eqn:histogram_binning}
  \min_{\theta_{1}, \ldots, \theta_{M}} &\sum_{m=1}^{M} \sum_{i=1}^{n} \mathbf{1}\left(a_{m} \leq \hat{p}_{i}<a_{m+1}\right)\left(\theta_{m}-y_{i}\right)^{2} \\ \nonumber
  \text{where}~\hat{p}_{i} &= \text{The uncalibrated probability of sample $i$ being positive (1)}
  M &= \text{The number of bins}, \\ \nonumber
  n &= \text{The number of samples}, \\ \nonumber
  1 &= \text{The indicator function}, \\ \nonumber
  0=a_{1} \leq a_{2} \leq \ldots \leq a_{M+1}=1 &= \parbox[t]{5cm}{The boundaries of bin $B_{m}$ defined by the interval $\left(a_{m}, a_{m+1}\right]$,} \\ \nonumber
  \theta_{m} &= \parbox[t]{5cm}{The calibrated prediction confidence associated with bin $m$,} \\ \nonumber
  y_{i} &= \parbox[t]{5cm}{Sample $i$'s true class, either positive (1) or negative (0)}
\end{align}
Typically the bins are either made equally long or sized to equalize the number of samples they contain.
Given fixed bin boundaries, the solution to equation \ref{eqn:histogram_binning} is to let $\theta_{m}$ be equal to the average number of positive-class samples in bin $B_{m}$.

\subsubsection{Isotonic regression}
Isotonic regression is a generalization of histogram binning where not only the calibrated prediction confidences of each bin but also the bin boundaries are optimized, as defined by equation \ref{eqn:isotonic_regression} in the case of binary classification.
\begin{align}
  \label{eqn:isotonic_regression}
  \min _{
    \begin{array}{c}
      M \\
	  \genfrac{}{}{0pt}{}{\theta_{1}, \ldots, \theta_{M}}{a_{1}, \ldots, a_{M+1}}
    \end{array}
  } & \sum_{m = 1}^{M} \sum_{i = 1}^{n} 1\left(a_{m} \leq \hat{p}_{i} < a_{m + 1}\right)\left(\theta_{m} - y_{i}\right)^{2} \\ \nonumber
  \text{ subject to } & 0 = a_{1} \leq a_{2} \leq \ldots \leq a_{M + 1}=1, \\
  & \theta_{1} \leq \theta_{2} \leq \ldots \leq \theta_{M}. \\ \nonumber
  \text{where}~\hat{p}_{i} &= \text{The uncalibrated probability of sample $i$ being positive (1)}
  M &= \text{The number of bins}, \\ \nonumber
  n &= \text{The number of samples}, \\ \nonumber
  1 &= \text{The indicator function}, \\ \nonumber
  0=a_{1} \leq a_{2} \leq \ldots \leq a_{M+1}=1 &= \parbox[t]{5cm}{The boundaries of bin $B_{m}$ defined by the interval $\left(a_{m}, a_{m+1}\right]$,} \\ \nonumber
  \theta_{m} &= \parbox[t]{5cm}{The calibrated prediction confidence associated with bin $m$,} \\ \nonumber
  y_{i} &= \parbox[t]{5cm}{Sample $i$'s true class, either positive (1) or negative (0)}
\end{align}

\subsubsection{Bayesian binning into quantiles (BBQ)}
Bayesian binning into quantiles (BBQ) is a Bayesian maximum a posteriori (MAP) calibrated probability estimation method that can be seen as a variation on histogram binning. Its basis is marginalization over the space of all possible binning schemes $\mathcal{S}$. We define a binning scheme $s$ as a pair $(M, \mathcal{I})$ where $M$ is the number of bins, and $\mathcal{I}$ is a corresponding partitioning of $[0,1]$ into $M$ disjoint intervals with boundaries $0=a_{1} \leq a_{2} \leq \ldots \leq a_{M+1}=1$. A binning scheme has parameters $\theta_{1}, \ldots, \theta_{M}$ defining a generative model for data on it. Equation \ref{eqn:bbq_pq} shows how a probability distribution of the calibrated confidence $\hat{q}$ conditioned on the data ($D$) and the uncalibrated confidence $\hat{p}$ can be defined as a marginalization over the space of all possible binning schemes. Equation \ref{eqn:bbq_pq_in_terms_of_s} shows

\begin{align}
\label{eqn:bbq_pq}
\mathbb{P}\left(\hat{q} \mid \hat{p}, D\right) &= \sum_{s \in \mathcal{S}} \mathbb{P}\left(\hat{q}, S=s \mid \hat{p}, D\right) \\
\label{eqn:bbq_pq_in_terms_of_s}
&= \sum_{s \in \mathcal{S}} \mathbb{P}\left(\hat{q} \mid \hat{p}, S=s, D\right) \mathbb{P}(S=s \mid D)
\end{align}

Using a uniform prior, the weight $\mathbb{P}(S=s \mid D)$ from equation \ref{eqn:bbq_pq_in_terms_of_s} can be derived using Bayes' rule:
\begin{equation}
\label{eqn:ps_given_d}
\mathbb{P}(S=s \mid D)=\frac{\mathbb{P}(D \mid S=s)}{\sum_{s^{\prime} \in \mathcal{S}} \mathbb{P}\left(D \mid S=s^{\prime}\right)}.
\end{equation}
By viewing parameters $\theta_{1}, \ldots, \theta_{M}$ as parameters of $M$ independent binomial distributions and by placing Beta priors on them, we can obtain a closed form expression for the marginal likelihood $\mathbb{P}(D \mid S=s)$. This allows us to backtrack through equations \ref{eqn:ps_given_d}, \ref{eqn:bbq_pq_in_terms_of_s} and \ref{eqn:bbq_pq} to finally compute $\mathbb{P}\left(\hat{q} \mid \hat{p}, D\right)$ for any test input.

Using equation \ref{eqn:bbq_pq} to \ref{eqn:ps_given_d}, we can use equation \ref{eqn:bbq_pq_argmax} to determine the calibrated confidence of the model given the data and its uncalibrated confidence. This is Bayesian binning into quantiles.
\begin{equation}
\label{eqn:bbq_pq_argmax}
\hat{q} = \argmax_{\hat{q}} \mathbb{P}\left(\hat{q} \mid \hat{p}, D\right)
\end{equation}

\subsubsection{Platt and temperature scaling}
\subsubsection{Comparison}
  
\begin{table}[h]
  \centering
  \resizebox{\linewidth}{!}{
    \begin{tabular}{ *{9}{c} }
      \hline
      Dataset & Model & Uncalibrated & Hist. Binning & Isotonic & BBQ & Temp. Scaling & Vector Scaling & Matrix Scaling \\
      \hline
      Birds & ResNet 50 & $9.19\%$ & $4.34\%$ & $5.22\%$ & $4.12\%$ & $1.85\%$ & $3.0\%$ & $21.13\%$ \\
      Cars & ResNet 50 & $4.3\%$ & $1.74\%$ & $4.29\%$ & $1.84\%$ & $2.35\%$ & $2.37\%$ & $10.5\%$
% CIFAR-10 & ResNet 110 & 4.6% 0.58% 0.81% 0.54% 0.83% 0.88% 1.0%
% CIFAR-10 & ResNet 110 (SD) & 4.12% 0.67% 1.11% 0.9% 0.6% 0.64% 0.72%
% CIFAR-10 & Wide ResNet 32 4.52% 0.72% 1.08% 0.74% 0.54% 0.6% 0.72%
% CIFAR-10 & DenseNet 40 3.28% 0.44% 0.61% 0.81% 0.33% 0.41% 0.41%
% CIFAR-10 & LeNet 5 3.02% 1.56% 1.85% 1.59% 0.93% 1.15% 1.16%
% CIFAR-100 & ResNet 110 16.53% 2.66% 4.99% 5.46% 1.26% 1.32% 25.49%
% CIFAR-100 & ResNet 110 (SD) 12.67% 2.46% 4.16% 3.58% 0.96% 0.9% 20.09%
% CIFAR-100 & Wide ResNet 32 & 15.0% 3.01% 5.85% 5.77% 2.32% 2.57% 24.44%
% CIFAR-100 & DenseNet 40 & 10.37% 2.68% 4.51% 3.59% 1.18% 1.09% 21.87%
% CIFAR-100 & LeNet 5 & 4.85% 6.48% 2.35% 3.77% 2.02% 2.09% 13.24%
% ImageNet & DenseNet 161 & 6.28% 4.52% 5.18% 3.51% 1.99% 2.24% -
% ImageNet & ResNet 152 & 5.48% 4.36% 4.77% 3.56% 1.86% 2.23% -
% SVHN & ResNet 152 (SD) & 0.44% 0.14% 0.28% 0.22% 0.17% 0.27% 0.17%

% Cars & ResNet 50 & $0.5488$ & $0.7977$ & $0.8793$ & $0.6986$ & $0.5311$ & $\mathbf{0 . 5 2 9 9}$ & $1.0206$ \\
% CIFAR-10 & ResNet 110 & $0.3285$ & $0.2532$ & $0.2237$ & $0.263$ & $0.2102$ & $0.2088$ & $\mathbf{0 . 2 0 4 8}$ \\
% CIFAR-10 & ResNet 110 (SD) & $0.2959$ & $0.2027$ & $0.1867$ & $0.2159$ & $0.1718$ & $\mathbf{0 . 1 7 0 9}$ & $0.1766$ \\
% CIFAR-10 & Wide ResNet 32 & $0.3293$ & $0.2778$ & $0.2428$ & $0.2774$ & $0.2283$ & $0.2275$ & $\mathbf{0 . 2 2 2 9}$ \\
% CIFAR-10 & DenseNet 40 & $0.2228$ & $0.212$ & $0.1969$ & $0.2087$ & $\mathbf{0 . 1 7 5 0}$ & $0.1757$ & $0.176$ \\
% CIFAR-10 & LeNet 5 & $0.4688$ & $0.529$ & $0.4757$ & $0.4984$ & $0.459$ & $\mathbf{0 . 4 5 6 8}$ & $0.4607$ \\
% CIFAR-100 & ResNet 110 & $1.4978$ & $1.4379$ & $1.207$ & $1.5466$ & $\mathbf{1 . 0 4 4 2}$ & $1.0485$ & $2.5637$ \\
% CIFAR-100 & ResNet 110 (SD) & $1.1157$ & $1.1985$ & $1.0317$ & $1.1982$ & $\mathbf{0 . 8 6 1 3}$ & $0.8655$ & $1.8182$ \\
% CIFAR-100 & Wide ResNet 32 & $1.3434$ & $1.4499$ & $1.2086$ & $1.459$ & $\mathbf{1 . 0 5 6 5}$ & $1.0648$ & $2.5507$ \\
% CIFAR-100 & DenseNet 40 & $1.0134$ & $1.2156$ & $1.0615$ & $1.1572$ & $0.9026$ & $\mathbf{0 . 9 0 1 1}$ & $1.9639$ \\
% CIFAR-100 & LeNet 5 & $1.6639$ & $2.2574$ & $1.8173$ & $1.9893$ & $\mathbf{1 . 6 5 6 0}$ & $1.6648$ & $2.1405$ \\
% ImageNet & DenseNet 161 & $0.9338$ & $1.4716$ & $1.1912$ & $1.4272$ & $0.8885$ & $\mathbf{0 . 8 8 7 9}$ &  \\
% ImageNet & ResNet 152 & $0.8961$ & $1.4507$ & $1.1859$ & $1.3987$ & $\mathbf{0 . 8 6 5 7}$ & $0.8742$ &  \\
% SVHN & ResNet 152 (SD) & $0.0842$ & $0.1137$ & $0.095$ & $0.1062$ & $\mathbf{0 . 0 8 2 1}$ & $0.0844$ &  \\
% \hline
% 20 News & DAN 3 & $0.7949$ & $1.0499$ & $0.8968$ & $0.9519$ & $0.7387$ & $0.0924$ &  \\
% Reuters & DAN 3 & $0.102$ & $0.2403$ & $0.1475$ & $0.1167$ & $0.0994$ & $\mathbf{0 . 7 2 9 6}$ & $0.0 .9089$ \\
% SST Binary & TreeLSTM & $0.3367$ & $0.2842$ & $0.2908$ & $0.2778$ & $\mathbf{0 . 2 7 3 9}$ & $\mathbf{0 . 2 7 3 9}$ &  \\
% SST Fine Grained & TreeLSTM & $1.1475$ & $1.1717$ & $1.1661$ & $1.149$ &  &  &  \\
    \end{tabular}
  }
\end{table}

\section{Review of applications of interest}
In this work we want to evaluate and analyze the impact of calibration on representative DeepProbLog use-cases where it presents a considerable issue. Unfortunately, these use-cases are not a given. To find and choose some, we first required that:
\begin{itemize}
  \item using PLP with neural predicates to tackle it is appropriate
  \item it is widely studied in neuro-symbolic integration research
  \item there is room to improve it through callibrating (parts of) models
\end{itemize}
If the first and third criterions are not met, there is no reason to use DeepProbLog and/or calibration and the knowledge we'd gain and features we'd create would not necessarily be relevant to our target audience. That's why we disregard most well-known AI toy problems such as playing chess, the N-queens problem and MNIST digit recognition and we start with reservations about using the DeepProbLog demonstration problems showcased by \cite{manhaeve2018deepproblog}. But we do require widely used challenges like the general AI toy problems so that our work's properties and performance can be easily compared to those of other approaches (\cite{russell2002artificial}), hence our second criterion. \par
We then performed a shallow literature scan of neuro-symbolic integration to find recurring themes and toy problems in this field of research. The scientific literature search engines Google Scholar (by Google) and Limo (by KU Leuven) were used to search for papers containing the keywords "neuro", "symbolic" and "integration" simultaneously. 63 random papers were chosen from the results. A breakdown of the problem categories these papers work in is given in table \ref{subject_breakdown}. Papers can work in multiple categories, the count column shows how many work in the category and the \% column shows what percentage of the 63 work in it.
\begin{table}[!htbp]
\centering
\begin{tabular}{ |c|c|c| }
 \hline
 \textbf{Category} & \textbf{Count} & \textbf{\%} \\
 \hline
 Acoustic information processing & 1 & 1.59\% \\
 \hline
 Actuarial science & 2 & 3.17\% \\
 \hline
 Natural sciences modelling & 2 & 3.17\% \\
 \hline
 Control theory & 4 & 6.35\% \\
 \hline
 Fraud detection & 1 & 1.59\% \\
 \hline
 Natural language processing & 7 & 11.11\% \\
 \hline
 Numerical analysis & 1 & 1.59\% \\
 \hline
 Robotics & 2 & 3.17\% \\
 \hline
 Social \& political sciences & 2 & 3.17\% \\
 \hline
 Symbolic knowledge \& structured data processing & 33 & 52.38\%  \\
 \hline
 Visual information processing & 16 & 25.40\%  \\
 \hline
\end{tabular}
\caption{Subfield breakdown of neuro-symbolic integration research}
\label{subject_breakdown}
\end{table}